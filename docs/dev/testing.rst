.. _testing:

===============================================================================
Testing PUDL
===============================================================================
Our test suite is primarily written using `pytest <https://pytest.org>`__.
The tests are all stored under the ``test/`` directory in the main repository,
and they are organized into 3 broad categories, each with its own subdirectory:

* **Software Unit Tests** (``test/unit/``) can be run in seconds and don't
  require any external data. They test the basic functionality of various
  functions and classes, often using minimal inline data structures that are
  specified in the test modules themselves.
* **Software Integration Tests** (``test/integration/``) test larger
  collections of functionality, including the interactions between different
  parts of the overall software system, and in some cases interactions with
  external systems, requiring network connectivity. The main thing our
  integration tests do is run the full PUDL data processing pipeline for the
  most recent year of data. This takes around 15 minutes.
* **Data Validations** (``test/validate/``) sanity check the PUDL outputs
  generated by the data processing pipeline. This helps us catch issues with
  the input data, and more subtle bugs that don't prevent the code from
  executing, but do have unintended or unexpected impacts on the output data.
  The data validation requires a fully populated PUDL database and is quite
  different from the other tests.

-------------------------------------------------------------------------------
Running tests with Tox
-------------------------------------------------------------------------------
We use `Tox <https://tox.readthedocs.io>`__ to coordinate running different
groups of tests together, and to make sure that the tests are run in a fresh
Python environment without any other packages that might have been installed
locally, but that won't necessarily be installed by PUDL users. Tox's behavior
is configured with the ``tox.ini`` file in the main repository directory. There
are several different "test environments" defined, to test different aspects
of the software, or to perform other actions like building the documentation.
To see a list of the available environments with short descriptions in the PUDL
repository run:

.. code-block:: console

    $ tox -av

    default environments:
    ci               -> Run all continuous integration (CI) checks & generate test coverage.

    additional environments:
    flake8           -> Run the full suite of flake8 linters on the PUDL codebase.
    pre_commit       -> Run git pre-commit hooks not covered by the other linters.
    bandit           -> Check the PUDL codebase for common insecure code patterns.
    linters          -> Run the pre-commit, flake8 and bandit linters.
    doc8             -> Check the documentation input files for syntactical correctness.
    docs             -> Remove old docs output and rebuild HTML from scratch with Sphinx
    unit             -> Run all the software unit tests.
    ferc1_solo       -> Test whether FERC 1 can be loaded into the PUDL database alone.
    integration      -> Run all software integration tests and process a full year of data.
    validate         -> Run all data validation tests. This requires a complete PUDL DB.
    ferc1_schema     -> Verify FERC Form 1 DB schema are compatible for all years.
    full_integration -> Run ETL and integration tests for all years and data sources.
    full             -> Run all CI checks, but for all years of data.
    build            -> Prepare Python source and binary packages for release.
    testrelease      -> Do a dry run of Python package release using the PyPI test server.
    release          -> Release the PUDL package to the production PyPI server.

Continuous Integration Tests
^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Our default tox test environment is ``ci`` -- which includes all of the tests
that will be run in continuous integration using a `GitHub Action
<https://github.com/features/actions>`__. You should run these tests before
pushing code to the repository or making a pull request. Because it's the
default test environment, it will be run if you call Tox without any
arguments:

.. code-block:: console

    $ tox

This is equivalent to:

.. code-block:: console

    $ tox -e ci

If the dependencies have been changed, or you recently ran the tests while on
another branch of the repository with other dependencies, you should tell Tox
to rebuild the software environment it operates in with the ``-r`` (aka
``--recreate``) flag:

.. code-block:: console

    $ tox -re ci

In addition to running the ``unit`` and ``integration`` tests, the CI test
environment lints the code and documentation input files, and uses Sphinx
build the documentation. It also generates a test coverage report. Running
the full set of CI tests takes 20-25 minutes, and requires a fair amount of
data. If you don't already have that data downloaded, it will be downloaded
automatically and put in your :doc:`local datastore </datastore>`

.. note::

  Locally the tests will run using whatever version of Python is part of your
  ``pudl-dev`` conda environment, but we have our CI set up to test on both
  Python 3.8 and 3.9 in parallel.

Software Unit and Integration Tests
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
To run the ``unit`` or ``integration`` tests on their own, you use the ``-e``
flag to choose those test environments explicitly:

.. code-block:: console

    $ tox -e unit

or:

.. code-block:: console

    $ tox -e integration

Full ETL Tests
^^^^^^^^^^^^^^
As mentioned above the CI tests process a single year of data. If you would
like to more exhaustively test the ETL process without affecting your
existing FERC 1 and PUDL databases, you can use the ``full`` test
environment:

.. code-block:: console

    $ tox -e full

This will process all years of data for the EIA and FERC datasets, and all
years of EPA CEMS data for a single state (Idaho). The ETL parameters for
this test are defined in ``test/settings/full-integration-tests.yml``


Running Other Commands with Tox
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
You can run any of the individual test environments that ``tox -av`` lists on
their own.  Not all of them literally run tests. For instance, to lint and
build the documentation you can run:

.. code-block:: console

    $ tox -e docs

To run all of the code and documentation linters, but not run any of the other
tests:

.. code-block:: console

    $ tox -e linters

Each of the test environments defined in ``tox.ini`` is just a collection of
dependencies and commands. To see what they consist of, you can open the file
in your text editor. Each section starts with ``[testenv:xxxxxx]`` and the
section called ``commands`` is a list of shell commands that that test
environment will run.

-------------------------------------------------------------------------------
Input Data for Integration Tests
-------------------------------------------------------------------------------
The software integration tests need a year's worth of input data to process. By
default they will look in your local PUDL datastore to find it. If the data
they need isn't available locally, they will download it from Zenodo and put it
in the local datastore.

However, if you're editing code that affects how the datastore works, you
probably don't want to risk contaminating your working datastore. You can
use a disposable temporary datastore instead by having Tox pass the
``--tmp-data`` flag in to ``pytest`` like this:

.. code-block:: console

   $ tox -e integration -- --tmp-data

The floating ``--`` isn't a typo, it tells Tox that you're done giving it
command line arguments, and that any additional arguments it gets should be
passed through to ``pytest``. We've configured ``pytest`` (through the
``test/conftest.py`` configuration file) to be on the lookout for the
``--tmp-data`` flag and act accordingly.

.. seealso::

    * :doc:`/usage` for more on how to set up a PUDL workspace, including a
      datastore.
    * :doc:`/datastore` for more on how to work with the datastore.

-------------------------------------------------------------------------------
Running pytest Directly
-------------------------------------------------------------------------------
Running tests directly with ``pytest`` gives you the ability to run only
tests from a particular test module, or even a single individual test case.
It's also faster because there's no testing environment to set up. Instead,
it just uses your Python environment, which should be the ``pudl-dev`` conda
environment discussed in :doc:`/dev/dev_setup`. This is convenient if you're
debugging something specific, or developing new test cases, but it's not as
robust as using Tox.

Running specific tests
^^^^^^^^^^^^^^^^^^^^^^
To run the software unit tests with ``pytest`` directly:

.. code-block:: console

   $ pytest test/unit

To run only the unit tests for the Excel spreadsheet extraction module:

.. code-block:: console

   $ pytest test/unit/extract/excel_test.py

To run only the unit tests defined by a single test class within that module:

.. code-block:: console

   $ pytest test/unit/extract/excel_test.py::TestGenericExtractor

Custom PUDL pytest flags
^^^^^^^^^^^^^^^^^^^^^^^^
We have defined several custom flags to control pytest's behavior when running
the PUDL tests. You can always check to see what custom flags exist by running
``pytest --help`` and looking at the ``custom options`` section:

.. code-block:: console

  custom options:
  --live-dbs            Use existing PUDL/FERC1 DBs instead of creating temporary ones.
  --tmp-data            Download fresh input data for use with this test run only.
  --etl-settings=ETL_SETTINGS
                        Path to a non-standard ETL settings file to use.
  --gcs-cache-path=GCS_CACHE_PATH
                        If set, use this GCS path as a datastore cache layer.
  --sandbox             Use raw inputs from the Zenodo sandbox server.

Run the output tests using a pre-existing PUDL database rather than building
a new one for testing:

.. code-block:: console

  $ pytest --live-dbs test/integration/fast_output_test.py

Run the ETL portion of the integration tests, and force it to download fresh
input data to a temporary datastore:

.. code-block:: console

   $ pytest --tmp-data test/integration/etl_test.py

Run the ETL portion of the integration tests, but using a non-standard settings
file (equivalent to ``tox -e ferc1_solo``):

.. code-block:: console

  $ pytest --etl-settings=test/settings/ferc1-solo-test.yml test/integration/etl_test.py

Check that the schemas of all historical FERC 1 databases are compatible with
the most recent year, which we use to generate the schema for our FERC 1 SQLite
database containing all years of FERC 1 data (equivalent to ``tox -e ferc1_schema``):

.. code-block:: console

  $ pytest --etl-settings test/settings/full-integration-test.yml test/integration/etl_test.py::test_ferc1_schema

-------------------------------------------------------------------------------
Data Validation
-------------------------------------------------------------------------------
Once the PUDL ETL pipeline has processed all of the available data, we have a
collection of tests that can be run to verify a collection of expectations
about what the outputs should look like. These data validation tests are
organized into datasource specific modules under ``test/validate``. Running
the full data validation can take as much as an hour, depending on your
computer.

These tests require a fully populated PUDL database which contains all
available FERC and EIA data, as specified by the
``test/settings/full-integration-test.yml`` input file. They are run against the
"live" SQLite database in your pudl workspace at ``sqlite/pudl.sqlite``. To run
the full data validation against an existing database:

.. code-block:: console

    $ tox -e validate

Which internally calls pytest like this, but in a clean Python environment:

.. code-block:: console

    $ pytest --live-dbs test/validate

The data validation cases that pertain to the contents of the data tables are
currently stored as part of the :mod:`pudl.validate` module.

The expected number of records in each output table is stored in the validation
test modules under ``test/validate`` as pytest parameterizations.

Data Validation Notebooks
^^^^^^^^^^^^^^^^^^^^^^^^^
We also have a collection of Jupyter Notebooks that use the same functions as
the data validation tests and also produce some visualizations of the data to
make it easier to understand what's wrong when validation fails. These
notebooks are stored in ``test/notebooks``

Like the data validations, the notebooks will only run successfully when
there's a full PUDL SQLite database available in your PUDL workspace.
