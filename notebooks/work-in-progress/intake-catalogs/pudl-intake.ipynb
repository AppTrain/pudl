{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Notebook Preamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IPython Magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Notebook Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "\n",
    "# 3rd Party Imports:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlalchemy as sa\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from intake import open_catalog\n",
    "\n",
    "# Local Imports\n",
    "import pudl\n",
    "from pudl.output.pudltabl import PudlTabl\n",
    "from pudl.metadata.classes import Resource\n",
    "from pudl.output.epacems import year_state_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler(stream=sys.stdout)\n",
    "formatter = logging.Formatter(\"%(message)s\")\n",
    "handler.setFormatter(formatter)\n",
    "logger.handlers = [handler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up standard PUDL DB connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pudl_settings = pudl.workspace.setup.get_defaults()\n",
    "ferc1_engine = sa.create_engine(pudl_settings[\"ferc1_db\"])\n",
    "pudl_engine = sa.create_engine(pudl_settings[\"pudl_db\"])\n",
    "\n",
    "pudl_out_raw = pudl.output.pudltabl.PudlTabl(pudl_engine=pudl_engine)\n",
    "pudl_out = pudl_out_raw\n",
    "\n",
    "pudl_settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Potential benefits of Intake catalogs:\n",
    "**Expose metadata:** The Intake catalog doesn't contain any column-level metadata, but I think it could. This would allow a user to see what columns were available, what their types were, and read their descriptions before querying the large dataset.\n",
    "\n",
    "**Local data caching:** Local file caching is not available. We would expect this to make using many small files more efficient for repeated access, since they would each only need to be transmitted over the network once. However, `fsspec` based file caching hasn't yet been implemented in the `intake-parquet` library.\n",
    "\n",
    "**Data packaging:** The catalog can be packaged and versioned using conda to manage its dependencies on other software packages and ensure compatibility. With remote access or automatic local file caching, the user also doesn't need to think about where the data is being stored, or putting it in the \"right place\" -- Intake would manage that.\n",
    "\n",
    "**Uniform API:** All the data sources of a given type (parquet, SQL) would have the same interface, reducing the number of things a user needs to remember to access the data.\n",
    "\n",
    "**Decoupling data storage location:** As with DNS, we can change / update the location where the data is being stored without impacting the user directly, since the catalog acts as a decoupling reference.\n",
    "\n",
    "## Intake References\n",
    "* [Intake Documentations](https://intake.readthedocs.io/en/latest/start.html)\n",
    "* [Intake Examples](https://github.com/intake/intake-examples)\n",
    "* [CarbonPlan Data Catalogs](https://github.com/carbonplan/data)\n",
    "* [AnacondaCon Presentation Video](https://www.youtube.com/watch?v=oyZJrROQzUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Intake & Parquet Functionality & Performance\n",
    "\n",
    "This notebook demonstrates several different ways of organizing and accessing the same EPA CEMS data:\n",
    "* Local storage on disk vs. remote storage in Google Cloud Storage buckets\n",
    "* Directly accessing the data via `pandas.read_parquet()` vs. an Intake catalog.\n",
    "* Using one big Parquet file for all data vs. separate small files for each combination of state & year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPACEMS_DIR = pudl_settings[\"parquet_dir\"] + \"/epacems\"\n",
    "\n",
    "TEST_FILTERS = year_state_filter(years=[2019, 2020], states=[\"CO\", \"TX\", \"ID\"])\n",
    "INTAKE_PATH_LOCAL = Path(os.getcwd())\n",
    "os.environ[\"INTAKE_PATH\"] = str(INTAKE_PATH_LOCAL)\n",
    "\n",
    "# Authenticated URL:\n",
    "INTAKE_PATH_REMOTE = \"gs://catalyst.coop/intake/test\"\n",
    "# Publicly visible URL:\n",
    "#INTAKE_PATH_REMOTE = \"https://storage.googleapis.com/catalyst.coop/intake/test\"\n",
    "\n",
    "local_single_file = str(INTAKE_PATH_LOCAL / \"hourly_emissions_epacems.parquet\")\n",
    "local_multi_file = str(INTAKE_PATH_LOCAL / \"hourly_emissions_epacems\")\n",
    "remote_single_file = INTAKE_PATH_REMOTE + \"/hourly_emissions_epacems.parquet\"\n",
    "remote_multi_file = INTAKE_PATH_REMOTE + \"/hourly_emissions_epacems\"\n",
    "\n",
    "pudl_catalog_path = str(INTAKE_PATH_LOCAL / \"pudl-catalog.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pudl_cat = open_catalog(pudl_catalog_path)\n",
    "list(pudl_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pudl_cat.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source level description\n",
    "* Basically from the YAML file, but with some extra things, `container`, `direct_access`, `user_parameters`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pudl_cat.epacems_one_file.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source internals\n",
    "* Categorical values showing up as integers.\n",
    "* String values showing up as objects.\n",
    "* No length in the shape, but 19 columns.\n",
    "* `npartitions` is apparently referring to file, not row-group based partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pudl_cat.epacems_one_file.discover()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The other source internals\n",
    "* Here we have nullable ints, but they're 64-bit?\n",
    "* Categories show up as `category` not integers.\n",
    "* Strings show up as `string` not `object`\n",
    "* `npartitions` is referring to the separate files. How do we get information about how the partitions are structured in here?\n",
    "* Somehow in `shape` we've lost a couple of columns! There's no state or year. Probably this is because of how I converted from the old hive partitioned version of epacems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pudl_cat.epacems_multi_file.discover()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PUDL Baseline\n",
    "Read the test data from your local EPA CEMS outputs directly.\n",
    "* On an SSD this should take less than 10 seconds.\n",
    "* The only `string` type columns should be `unitid` and `unit_id_epa`\n",
    "* The dataframe should take about 1.4 GB of memory and have ~8M rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pudl_epacems = pd.read_parquet(\n",
    "    EPACEMS_DIR,\n",
    "    engine=\"pyarrow\",\n",
    "    filters=TEST_FILTERS,\n",
    "    use_nullable_dtypes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pudl_epacems.info(show_counts=True, memory_usage=\"deep\"))\n",
    "del pudl_epacems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single File Local\n",
    "For the single file local tests, download [this file](https://storage.googleapis.com/catalyst.coop/intake/test/hourly_emissions_epacems.parquet) into the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct access with `read_parquet()`\n",
    "* This takes 3-4 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_parquet(\n",
    "    local_single_file,\n",
    "    engine=\"pyarrow\",\n",
    "    filters=TEST_FILTERS,\n",
    "    use_nullable_dtypes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.info(show_counts=True, memory_usage=\"deep\"))\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Via Intake Catalog\n",
    "* This takes 3-4 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "os.environ[\"INTAKE_PATH\"] = str(INTAKE_PATH_LOCAL)\n",
    "pudl_cat = open_catalog(pudl_catalog_path)\n",
    "source = pudl_cat.epacems_one_file(filters=TEST_FILTERS)\n",
    "dd = source.to_dask()\n",
    "df = dd.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.info(show_counts=True, memory_usage=\"deep\"))\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single File Remote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct access with `read_parquet()`\n",
    "* Using the authenticated `gs://` URL it takes **20 seconds**\n",
    "* Using the public `https://` URL this takes **10+ minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_parquet(\n",
    "    remote_single_file,\n",
    "    engine=\"pyarrow\",\n",
    "    filters=TEST_FILTERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.info(show_counts=True, memory_usage=\"deep\"))\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Via Intake Catalog\n",
    "* With `gs://` URL this takes **1 minute**\n",
    "* With `https://` URL it downloads a huge amount of data and then times out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "os.environ[\"INTAKE_PATH\"] = INTAKE_PATH_REMOTE\n",
    "pudl_cat = open_catalog(pudl_catalog_path)\n",
    "source = pudl_cat.epacems_one_file(filters=TEST_FILTERS)\n",
    "dd = source.to_dask()\n",
    "df = dd.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.info(show_counts=True, memory_usage=\"deep\"))\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi File Local\n",
    "\n",
    "For the multi-file local tests download [this tarball](https://storage.googleapis.com/catalyst.coop/intake/test/hourly_emissions_epacems.tar) and extract it in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct access with `read_parquet()`\n",
    "* This takes 5 seconds, and results in an excessively large 3GB dataframe because I generated these parquet files before fixing the string-to-categorical type issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_parquet(\n",
    "    local_multi_file,\n",
    "    engine=\"pyarrow\",\n",
    "    filters=TEST_FILTERS,\n",
    "    use_nullable_dtypes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.info(show_counts=True, memory_usage=\"deep\"))\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Via Intake Catalog\n",
    "* This takes about 15 seconds, and results in the 3GB dataframe as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "os.environ[\"INTAKE_PATH\"] = str(INTAKE_PATH_LOCAL)\n",
    "pudl_cat = open_catalog(pudl_catalog_path)\n",
    "source = pudl_cat.epacems_multi_file(\n",
    "    filters=TEST_FILTERS,\n",
    "    engine=\"pyarrow\",\n",
    ")\n",
    "dd = source.to_dask()\n",
    "df = dd.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.info(show_counts=True, memory_usage=\"deep\"))\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi File Remote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct access with `read_parquet()`\n",
    "* With the `gs://` URL this takes **1 minute** and downloads minimal data.\n",
    "* With the `https://` URL this results in a 403 Forbidden error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_parquet(\n",
    "    remote_multi_file,\n",
    "    engine=\"pyarrow\",\n",
    "    filters=TEST_FILTERS,\n",
    "    use_nullable_dtypes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.info(show_counts=True, memory_usage=\"deep\"))\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Via Intake Catalog\n",
    "* With the `gs://` URL this takes **3 minutes** and downloads a little bit of data across the whole time.\n",
    "* With the `https://` URL this results in a 403 Forbidden error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "os.environ[\"INTAKE_PATH\"] = INTAKE_PATH_REMOTE\n",
    "pudl_cat = open_catalog(pudl_catalog_path)\n",
    "source = pudl_cat.epacems_multi_file(\n",
    "    filters=TEST_FILTERS,\n",
    "    engine=\"pyarrow\",\n",
    ")\n",
    "dd = source.to_dask()\n",
    "df = dd.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.info(show_counts=True, memory_usage=\"deep\"))\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "* Unsurprisingly, local access is blazing fast regardless of whether it's a single file or many, and while the Intake catalog access takes around 3x as long, it seems fast enough to be plenty usable.\n",
    "* Remote performance using a single file, the `gs://` protocol, and `read_parquet()` was shockingly fast. It took less than 10x as long as direct local access.\n",
    "* Remote performance over `https://` was painfully slow, to the point of being unusable in all uses of Intake. It also seemed to be transmitting far, far more data than in the `gs://` case.\n",
    "* Basically none of the `https://` cases were usable. The only one that completed took 10 minutes.\n",
    "* The only remote Intake catalog case that worked was the single-file `gs://`, which (as with the local catalogs) took about 3x as long as the `read_parquet()` case.\n",
    "* Over `https://` it seems like we can't use directories or wildcards -- we have to enumerate each filename specifically.\n",
    "* Some of the issues here have to be network speed, but I have 50-100Mbit download speeds, and the amount of data being transmitted varied widely between the different cases.\n",
    "* Still some data type issues happening in all of the Intake cases. Strings get turned into objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "* Can non-authenticated users access publicly readable data using `gs://` URLs?\n",
    "* How do we add column-level metadata to the catalog appropriately? Can we get the embedded descriptions to show up?\n",
    "* How do we add information about what's in the different partitions (i.e. split by year and state, allowable values)\n",
    "* Why are we getting jumbled nullable/non-nullable, ints/categories, strings/objects in the types?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
