{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Notebook Preamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IPython Magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Notebook Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "\n",
    "# 3rd Party Imports:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlalchemy as sa\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from intake import open_catalog\n",
    "\n",
    "# Local Imports\n",
    "import pudl\n",
    "from pudl.output.pudltabl import PudlTabl\n",
    "from pudl.metadata.classes import Resource\n",
    "from pudl.output.epacems import year_state_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler(stream=sys.stdout)\n",
    "formatter = logging.Formatter(\"%(message)s\")\n",
    "handler.setFormatter(formatter)\n",
    "logger.handlers = [handler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up standard PUDL DB connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pudl_in': '/home/zane/code/catalyst/pudl-work',\n",
       " 'data_dir': '/home/zane/code/catalyst/pudl-work/data',\n",
       " 'settings_dir': '/home/zane/code/catalyst/pudl-work/settings',\n",
       " 'pudl_out': '/home/zane/code/catalyst/pudl-work',\n",
       " 'sqlite_dir': '/home/zane/code/catalyst/pudl-work/sqlite',\n",
       " 'parquet_dir': '/home/zane/code/catalyst/pudl-work/parquet',\n",
       " 'ferc1_db': 'sqlite:////home/zane/code/catalyst/pudl-work/sqlite/ferc1.sqlite',\n",
       " 'pudl_db': 'sqlite:////home/zane/code/catalyst/pudl-work/sqlite/pudl.sqlite',\n",
       " 'censusdp1tract_db': 'sqlite:////home/zane/code/catalyst/pudl-work/sqlite/censusdp1tract.sqlite'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pudl_settings = pudl.workspace.setup.get_defaults()\n",
    "ferc1_engine = sa.create_engine(pudl_settings[\"ferc1_db\"])\n",
    "pudl_engine = sa.create_engine(pudl_settings[\"pudl_db\"])\n",
    "\n",
    "pudl_out_raw = pudl.output.pudltabl.PudlTabl(pudl_engine=pudl_engine)\n",
    "pudl_out = pudl_out_raw\n",
    "\n",
    "pudl_settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Potential benefits of Intake catalogs:\n",
    "**Expose metadata:** The Intake catalog doesn't contain any column-level metadata, but I think it could. This would allow a user to see what columns were available, what their types were, and read their descriptions before querying the large dataset.\n",
    "\n",
    "**Local data caching:** Local file caching is not available. We would expect this to make using many small files more efficient for repeated access, since they would each only need to be transmitted over the network once. However, `fsspec` based file caching hasn't yet been implemented in the `intake-parquet` library.\n",
    "\n",
    "**Data packaging:** The catalog can be packaged and versioned using conda to manage its dependencies on other software packages and ensure compatibility. With remote access or automatic local file caching, the user also doesn't need to think about where the data is being stored, or putting it in the \"right place\" -- Intake would manage that.\n",
    "\n",
    "**Uniform API:** All the data sources of a given type (parquet, SQL) would have the same interface, reducing the number of things a user needs to remember to access the data.\n",
    "\n",
    "**Decoupling data storage location:** As with DNS, we can change / update the location where the data is being stored without impacting the user directly, since the catalog acts as a decoupling reference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Intake & Parquet Functionality & Performance\n",
    "\n",
    "This notebook demonstrates several different ways of organizing and accessing the same EPA CEMS data:\n",
    "* Local storage on disk vs. remote storage in Google Cloud Storage buckets\n",
    "* Directly accessing the data via `pandas.read_parquet()` vs. an Intake catalog.\n",
    "* Using one big Parquet file for all data vs. separate small files for each combination of state & year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPACEMS_DIR = pudl_settings[\"parquet_dir\"] + \"/epacems\"\n",
    "\n",
    "TEST_FILTERS = year_state_filter(years=[2019, 2020], states=[\"CO\", \"TX\", \"ID\"])\n",
    "INTAKE_PATH_LOCAL = Path(os.getcwd())\n",
    "\n",
    "# Authenticated URL:\n",
    "INTAKE_PATH_REMOTE = \"gs://catalyst.coop/intake/test\"\n",
    "# Publicly visible URL:\n",
    "#INTAKE_PATH_REMOTE = \"https://storage.googleapis.com/catalyst.coop/intake/test\"\n",
    "\n",
    "local_single_file = str(INTAKE_PATH_LOCAL / \"hourly_emissions_epacems.parquet\")\n",
    "local_multi_file = str(INTAKE_PATH_LOCAL / \"hourly_emissions_epacems\")\n",
    "remote_single_file = INTAKE_PATH_REMOTE + \"/hourly_emissions_epacems.parquet\"\n",
    "remote_multi_file = INTAKE_PATH_REMOTE + \"/hourly_emissions_epacems\"\n",
    "\n",
    "pudl_catalog_path = str(INTAKE_PATH_LOCAL / \"pudl-catalog.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PUDL Baseline\n",
    "Read the test data from your local EPA CEMS outputs directly.\n",
    "* On an SSD this should take less than 10 seconds.\n",
    "* The only `string` type columns should be `unitid` and `unit_id_epa`\n",
    "* The dataframe should take about 1.4 GB of memory and have ~8M rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.34 s, sys: 1.26 s, total: 5.61 s\n",
      "Wall time: 3.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pudl_epacems = pd.read_parquet(\n",
    "    EPACEMS_DIR,\n",
    "    engine=\"pyarrow\",\n",
    "    filters=TEST_FILTERS,\n",
    "    use_nullable_dtypes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8006424 entries, 0 to 8006423\n",
      "Data columns (total 19 columns):\n",
      " #   Column                     Non-Null Count    Dtype              \n",
      "---  ------                     --------------    -----              \n",
      " 0   state                      8006424 non-null  category           \n",
      " 1   plant_id_eia               8006424 non-null  Int32              \n",
      " 2   unitid                     8006424 non-null  string             \n",
      " 3   operating_datetime_utc     8006424 non-null  datetime64[ns, UTC]\n",
      " 4   operating_time_hours       8003928 non-null  float32            \n",
      " 5   gross_load_mw              8006424 non-null  float32            \n",
      " 6   steam_load_1000_lbs        33252 non-null    float32            \n",
      " 7   so2_mass_lbs               3586052 non-null  float32            \n",
      " 8   so2_mass_measurement_code  3586052 non-null  category           \n",
      " 9   nox_rate_lbs_mmbtu         3716001 non-null  float32            \n",
      " 10  nox_rate_measurement_code  3716001 non-null  category           \n",
      " 11  nox_mass_lbs               3716549 non-null  float32            \n",
      " 12  nox_mass_measurement_code  3716549 non-null  category           \n",
      " 13  co2_mass_tons              3688397 non-null  float32            \n",
      " 14  co2_mass_measurement_code  3688397 non-null  category           \n",
      " 15  heat_content_mmbtu         8006424 non-null  float32            \n",
      " 16  facility_id                8006424 non-null  Int32              \n",
      " 17  unit_id_epa                8006424 non-null  string             \n",
      " 18  year                       8006424 non-null  Int32              \n",
      "dtypes: Int32(3), category(5), datetime64[ns, UTC](1), float32(8), string(2)\n",
      "memory usage: 1.4 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pudl_epacems.info(show_counts=True, memory_usage=\"deep\"))\n",
    "del pudl_epacems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single File Local\n",
    "For the single file local tests, download [this file](https://storage.googleapis.com/catalyst.coop/intake/test/hourly_emissions_epacems.parquet) into the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct access with `read_parquet()`\n",
    "* This takes 3-4 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.04 s, sys: 1.19 s, total: 5.23 s\n",
      "Wall time: 4.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_parquet(\n",
    "    local_single_file,\n",
    "    engine=\"pyarrow\",\n",
    "    filters=TEST_FILTERS,\n",
    "    use_nullable_dtypes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8006424 entries, 0 to 8006423\n",
      "Data columns (total 19 columns):\n",
      " #   Column                     Non-Null Count    Dtype              \n",
      "---  ------                     --------------    -----              \n",
      " 0   state                      8006424 non-null  category           \n",
      " 1   plant_id_eia               8006424 non-null  Int32              \n",
      " 2   unitid                     8006424 non-null  string             \n",
      " 3   operating_datetime_utc     8006424 non-null  datetime64[ns, UTC]\n",
      " 4   operating_time_hours       8003928 non-null  float32            \n",
      " 5   gross_load_mw              8006424 non-null  float32            \n",
      " 6   steam_load_1000_lbs        33252 non-null    float32            \n",
      " 7   so2_mass_lbs               3586052 non-null  float32            \n",
      " 8   so2_mass_measurement_code  3586052 non-null  category           \n",
      " 9   nox_rate_lbs_mmbtu         3716001 non-null  float32            \n",
      " 10  nox_rate_measurement_code  3716001 non-null  category           \n",
      " 11  nox_mass_lbs               3716549 non-null  float32            \n",
      " 12  nox_mass_measurement_code  3716549 non-null  category           \n",
      " 13  co2_mass_tons              3688397 non-null  float32            \n",
      " 14  co2_mass_measurement_code  3688397 non-null  category           \n",
      " 15  heat_content_mmbtu         8006424 non-null  float32            \n",
      " 16  facility_id                8006424 non-null  Int32              \n",
      " 17  unit_id_epa                8006424 non-null  string             \n",
      " 18  year                       8006424 non-null  Int32              \n",
      "dtypes: Int32(3), category(5), datetime64[ns, UTC](1), float32(8), string(2)\n",
      "memory usage: 1.4 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.info(show_counts=True, memory_usage=\"deep\"))\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Via Intake Catalog\n",
    "* This takes 10-15 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.9 s, sys: 1.95 s, total: 12.9 s\n",
      "Wall time: 11.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "os.environ[\"INTAKE_PATH\"] = str(INTAKE_PATH_LOCAL)\n",
    "pudl_cat = open_catalog(pudl_catalog_path)\n",
    "source = pudl_cat.epacems_one_file(\n",
    "    filters=TEST_FILTERS,\n",
    "    engine=\"pyarrow\",\n",
    "    use_nullable_dtypes=True,\n",
    ")\n",
    "dd = source.to_dask()\n",
    "df = dd.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8006424 entries, 0 to 8006423\n",
      "Data columns (total 19 columns):\n",
      " #   Column                     Non-Null Count    Dtype              \n",
      "---  ------                     --------------    -----              \n",
      " 0   state                      8006424 non-null  category           \n",
      " 1   plant_id_eia               8006424 non-null  Int32              \n",
      " 2   unitid                     8006424 non-null  string             \n",
      " 3   operating_datetime_utc     8006424 non-null  datetime64[ns, UTC]\n",
      " 4   operating_time_hours       8003928 non-null  Float32            \n",
      " 5   gross_load_mw              8006424 non-null  Int64              \n",
      " 6   steam_load_1000_lbs        33252 non-null    Int64              \n",
      " 7   so2_mass_lbs               3586052 non-null  Float32            \n",
      " 8   so2_mass_measurement_code  3586052 non-null  category           \n",
      " 9   nox_rate_lbs_mmbtu         3716001 non-null  Float32            \n",
      " 10  nox_rate_measurement_code  3716001 non-null  category           \n",
      " 11  nox_mass_lbs               3716549 non-null  Float32            \n",
      " 12  nox_mass_measurement_code  3716549 non-null  category           \n",
      " 13  co2_mass_tons              3688397 non-null  Float32            \n",
      " 14  co2_mass_measurement_code  3688397 non-null  category           \n",
      " 15  heat_content_mmbtu         8006424 non-null  Float32            \n",
      " 16  facility_id                8006424 non-null  Int32              \n",
      " 17  unit_id_epa                8006424 non-null  string             \n",
      " 18  year                       8006424 non-null  Int32              \n",
      "dtypes: Float32(6), Int32(3), Int64(2), category(5), datetime64[ns, UTC](1), string(2)\n",
      "memory usage: 1.5 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.info(show_counts=True, memory_usage=\"deep\"))\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single File Remote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct access with `read_parquet()`\n",
    "* Using the authenticated `gs://` URL it takes **20 seconds**\n",
    "* Using the public `https://` URL this takes **10+ minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.71 s, sys: 1.52 s, total: 7.23 s\n",
      "Wall time: 19.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_parquet(\n",
    "    remote_single_file,\n",
    "    engine=\"pyarrow\",\n",
    "    filters=TEST_FILTERS,\n",
    "    use_nullable_dtypes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8006424 entries, 0 to 8006423\n",
      "Data columns (total 19 columns):\n",
      " #   Column                     Non-Null Count    Dtype              \n",
      "---  ------                     --------------    -----              \n",
      " 0   state                      8006424 non-null  category           \n",
      " 1   plant_id_eia               8006424 non-null  Int32              \n",
      " 2   unitid                     8006424 non-null  string             \n",
      " 3   operating_datetime_utc     8006424 non-null  datetime64[ns, UTC]\n",
      " 4   operating_time_hours       8003928 non-null  float32            \n",
      " 5   gross_load_mw              8006424 non-null  float32            \n",
      " 6   steam_load_1000_lbs        33252 non-null    float32            \n",
      " 7   so2_mass_lbs               3586052 non-null  float32            \n",
      " 8   so2_mass_measurement_code  3586052 non-null  category           \n",
      " 9   nox_rate_lbs_mmbtu         3716001 non-null  float32            \n",
      " 10  nox_rate_measurement_code  3716001 non-null  category           \n",
      " 11  nox_mass_lbs               3716549 non-null  float32            \n",
      " 12  nox_mass_measurement_code  3716549 non-null  category           \n",
      " 13  co2_mass_tons              3688397 non-null  float32            \n",
      " 14  co2_mass_measurement_code  3688397 non-null  category           \n",
      " 15  heat_content_mmbtu         8006424 non-null  float32            \n",
      " 16  facility_id                8006424 non-null  Int32              \n",
      " 17  unit_id_epa                8006424 non-null  string             \n",
      " 18  year                       8006424 non-null  Int32              \n",
      "dtypes: Int32(3), category(5), datetime64[ns, UTC](1), float32(8), string(2)\n",
      "memory usage: 1.4 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.info(show_counts=True, memory_usage=\"deep\"))\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Via Intake Catalog\n",
    "* With `gs://` URL this takes **1 minute**\n",
    "* With `https://` URL it downloads a huge amount of data and then times out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FSTimeoutError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/mambaforge/envs/pudl-dev/lib/python3.10/site-packages/fsspec/asyn.py:25\u001b[0m, in \u001b[0;36m_runner\u001b[0;34m(event, coro, result, timeout)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m coro\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "File \u001b[0;32m~/mambaforge/envs/pudl-dev/lib/python3.10/site-packages/fsspec/implementations/http.py:484\u001b[0m, in \u001b[0;36mHTTPFileSystem._isdir\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ls(path))\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mFileNotFoundError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n",
      "File \u001b[0;32m~/mambaforge/envs/pudl-dev/lib/python3.10/site-packages/fsspec/implementations/http.py:196\u001b[0m, in \u001b[0;36mHTTPFileSystem._ls\u001b[0;34m(self, url, detail, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ls_real(url, detail\u001b[38;5;241m=\u001b[39mdetail, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdircache[url] \u001b[38;5;241m=\u001b[39m out\n",
      "File \u001b[0;32m~/mambaforge/envs/pudl-dev/lib/python3.10/site-packages/fsspec/implementations/http.py:151\u001b[0m, in \u001b[0;36mHTTPFileSystem._ls_real\u001b[0;34m(self, url, detail, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_not_found_for_status(r, url)\n\u001b[0;32m--> 151\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m r\u001b[38;5;241m.\u001b[39mtext()\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimple_links:\n",
      "File \u001b[0;32m~/mambaforge/envs/pudl-dev/lib/python3.10/site-packages/aiohttp/client_reqrep.py:1080\u001b[0m, in \u001b[0;36mClientResponse.text\u001b[0;34m(self, encoding, errors)\u001b[0m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1080\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/pudl-dev/lib/python3.10/site-packages/aiohttp/client_reqrep.py:1036\u001b[0m, in \u001b[0;36mClientResponse.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1036\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m trace \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_traces:\n",
      "File \u001b[0;32m~/mambaforge/envs/pudl-dev/lib/python3.10/site-packages/aiohttp/streams.py:375\u001b[0m, in \u001b[0;36mStreamReader.read\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 375\u001b[0m     block \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreadany()\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m block:\n",
      "File \u001b[0;32m~/mambaforge/envs/pudl-dev/lib/python3.10/site-packages/aiohttp/streams.py:397\u001b[0m, in \u001b[0;36mStreamReader.readany\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof:\n\u001b[0;32m--> 397\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreadany\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_nowait(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/pudl-dev/lib/python3.10/site-packages/aiohttp/streams.py:303\u001b[0m, in \u001b[0;36mStreamReader._wait\u001b[0;34m(self, func_name)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timer:\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timer:\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m waiter\n",
      "File \u001b[0;32m~/mambaforge/envs/pudl-dev/lib/python3.10/site-packages/aiohttp/helpers.py:721\u001b[0m, in \u001b[0;36mTimerContext.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc_type \u001b[38;5;129;01mis\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mCancelledError \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cancelled:\n\u001b[0;32m--> 721\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mTimeoutError \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTimeoutError\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFSTimeoutError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:7\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m~/mambaforge/envs/pudl-dev/lib/python3.10/site-packages/intake_parquet/source.py:99\u001b[0m, in \u001b[0;36mParquetSource.to_dask\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dask\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df\n",
      "File \u001b[0;32m~/mambaforge/envs/pudl-dev/lib/python3.10/site-packages/intake/source/base.py:236\u001b[0m, in \u001b[0;36mDataSourceBase._load_metadata\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;124;03m\"\"\"load metadata only if needed\"\"\"\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/mambaforge/envs/pudl-dev/lib/python3.10/site-packages/intake_parquet/source.py:60\u001b[0m, in \u001b[0;36mParquetSource._get_schema\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_schema\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_dask\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     dtypes \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28mstr\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df\u001b[38;5;241m.\u001b[39m_meta\u001b[38;5;241m.\u001b[39mdtypes\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mSchema(datashape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     63\u001b[0m                                dtype\u001b[38;5;241m=\u001b[39mdtypes,\n\u001b[1;32m     64\u001b[0m                                shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df\u001b[38;5;241m.\u001b[39mcolumns)),\n\u001b[1;32m     65\u001b[0m                                npartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df\u001b[38;5;241m.\u001b[39mnpartitions,\n\u001b[1;32m     66\u001b[0m                                extra_metadata\u001b[38;5;241m=\u001b[39m{})\n",
      "File \u001b[0;32m~/mambaforge/envs/pudl-dev/lib/python3.10/site-packages/intake_parquet/source.py:108\u001b[0m, in \u001b[0;36mParquetSource._to_dask\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdd\u001b[39;00m\n\u001b[1;32m    107\u001b[0m urlpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_cache(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_urlpath)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df \u001b[38;5;241m=\u001b[39m \u001b[43mdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43murlpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_storage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_metadata()\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df\n",
      "File \u001b[0;32m~/mambaforge/envs/pudl-dev/lib/python3.10/site-packages/dask/dataframe/io/parquet/core.py:389\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, columns, filters, categories, index, storage_options, engine, gather_statistics, ignore_metadata_file, metadata_task_size, split_row_groups, chunksize, aggregate_files, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread_parquet options require gather_statistics=True\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    387\u001b[0m     gather_statistics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 389\u001b[0m read_metadata_result \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgather_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgather_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_row_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_row_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43maggregate_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregate_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_metadata_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_metadata_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata_task_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_task_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;66;03m# In the future, we may want to give the engine the\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;66;03m# option to return a dedicated element for `common_kwargs`.\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;66;03m# However, to avoid breaking the API, we just embed this\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;66;03m# data in the first element of `parts` for now.\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# The logic below is inteded to handle backward and forward\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;66;03m# compatibility with a user-defined engine.\u001b[39;00m\n\u001b[1;32m    410\u001b[0m meta, statistics, parts, index \u001b[38;5;241m=\u001b[39m read_metadata_result[:\u001b[38;5;241m4\u001b[39m]\n",
      "File \u001b[0;32m~/mambaforge/envs/pudl-dev/lib/python3.10/site-packages/dask/dataframe/io/parquet/arrow.py:366\u001b[0m, in \u001b[0;36mArrowDatasetEngine.read_metadata\u001b[0;34m(cls, fs, paths, categories, index, gather_statistics, filters, split_row_groups, chunksize, aggregate_files, ignore_metadata_file, metadata_task_size, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_metadata\u001b[39m(\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    364\u001b[0m \n\u001b[1;32m    365\u001b[0m     \u001b[38;5;66;03m# Stage 1: Collect general dataset information\u001b[39;00m\n\u001b[0;32m--> 366\u001b[0m     dataset_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collect_dataset_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgather_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_row_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43maggregate_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_metadata_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata_task_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;66;03m# Stage 2: Generate output `meta`\u001b[39;00m\n\u001b[1;32m    382\u001b[0m     meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dd_meta(dataset_info)\n",
      "File \u001b[0;32m~/mambaforge/envs/pudl-dev/lib/python3.10/site-packages/dask/dataframe/io/parquet/arrow.py:843\u001b[0m, in \u001b[0;36mArrowDatasetEngine._collect_dataset_info\u001b[0;34m(cls, paths, fs, categories, index, gather_statistics, filters, split_row_groups, chunksize, aggregate_files, ignore_metadata_file, metadata_task_size, kwargs)\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;66;03m# Case-dependent pyarrow.dataset creation\u001b[39;00m\n\u001b[1;32m    842\u001b[0m has_metadata_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 843\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(paths) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    844\u001b[0m \n\u001b[1;32m    845\u001b[0m     \u001b[38;5;66;03m# Use _analyze_paths to avoid relative-path\u001b[39;00m\n\u001b[1;32m    846\u001b[0m     \u001b[38;5;66;03m# problems (see GH#5608)\u001b[39;00m\n\u001b[1;32m    847\u001b[0m     paths, base, fns \u001b[38;5;241m=\u001b[39m _sort_and_analyze_paths(paths, fs)\n\u001b[1;32m    848\u001b[0m     paths \u001b[38;5;241m=\u001b[39m fs\u001b[38;5;241m.\u001b[39msep\u001b[38;5;241m.\u001b[39mjoin([base, fns[\u001b[38;5;241m0\u001b[39m]])\n",
      "File \u001b[0;32m~/mambaforge/envs/pudl-dev/lib/python3.10/site-packages/fsspec/asyn.py:85\u001b[0m, in \u001b[0;36msync_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m obj \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/pudl-dev/lib/python3.10/site-packages/fsspec/asyn.py:63\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m return_result \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_result, asyncio\u001b[38;5;241m.\u001b[39mTimeoutError):\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# suppress asyncio.TimeoutError, raise FSTimeoutError\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FSTimeoutError \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreturn_result\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_result, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m return_result\n",
      "\u001b[0;31mFSTimeoutError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "os.environ[\"INTAKE_PATH\"] = INTAKE_PATH_REMOTE\n",
    "pudl_cat = open_catalog(pudl_catalog_path)\n",
    "source = pudl_cat.epacems_one_file(\n",
    "    filters=TEST_FILTERS,\n",
    "    engine=\"pyarrow\",\n",
    ")\n",
    "dd = source.to_dask()\n",
    "df = dd.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8006424 entries, 0 to 8006423\n",
      "Data columns (total 19 columns):\n",
      " #   Column                     Non-Null Count    Dtype              \n",
      "---  ------                     --------------    -----              \n",
      " 0   state                      8006424 non-null  category           \n",
      " 1   plant_id_eia               8006424 non-null  int32              \n",
      " 2   unitid                     8006424 non-null  object             \n",
      " 3   operating_datetime_utc     8006424 non-null  datetime64[ns, UTC]\n",
      " 4   operating_time_hours       8003928 non-null  float32            \n",
      " 5   gross_load_mw              8006424 non-null  float32            \n",
      " 6   steam_load_1000_lbs        33252 non-null    float32            \n",
      " 7   so2_mass_lbs               3586052 non-null  float32            \n",
      " 8   so2_mass_measurement_code  3586052 non-null  category           \n",
      " 9   nox_rate_lbs_mmbtu         3716001 non-null  float32            \n",
      " 10  nox_rate_measurement_code  3716001 non-null  category           \n",
      " 11  nox_mass_lbs               3716549 non-null  float32            \n",
      " 12  nox_mass_measurement_code  3716549 non-null  category           \n",
      " 13  co2_mass_tons              3688397 non-null  float32            \n",
      " 14  co2_mass_measurement_code  3688397 non-null  category           \n",
      " 15  heat_content_mmbtu         8006424 non-null  float32            \n",
      " 16  facility_id                8006424 non-null  int32              \n",
      " 17  unit_id_epa                8006424 non-null  object             \n",
      " 18  year                       8006424 non-null  int32              \n",
      "dtypes: category(5), datetime64[ns, UTC](1), float32(8), int32(3), object(2)\n",
      "memory usage: 1.3 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.info(show_counts=True, memory_usage=\"deep\"))\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi File Local\n",
    "\n",
    "For the multi-file local tests download [this tarball](https://storage.googleapis.com/catalyst.coop/intake/test/hourly_emissions_epacems.tar) and extract it in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct access with `read_parquet()`\n",
    "* This takes 5 seconds, and results in an excessively large 3GB dataframe because I generated these parquet files before fixing the string-to-categorical type issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.27 s, sys: 1.43 s, total: 8.7 s\n",
      "Wall time: 5.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_parquet(\n",
    "    local_multi_file,\n",
    "    engine=\"pyarrow\",\n",
    "    filters=TEST_FILTERS,\n",
    "    use_nullable_dtypes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8006424 entries, 0 to 8006423\n",
      "Data columns (total 19 columns):\n",
      " #   Column                     Non-Null Count    Dtype              \n",
      "---  ------                     --------------    -----              \n",
      " 0   plant_id_eia               8006424 non-null  Int32              \n",
      " 1   unitid                     8006424 non-null  string             \n",
      " 2   operating_datetime_utc     8006424 non-null  datetime64[ns, UTC]\n",
      " 3   operating_time_hours       8003928 non-null  float32            \n",
      " 4   gross_load_mw              8006424 non-null  float32            \n",
      " 5   steam_load_1000_lbs        33252 non-null    float32            \n",
      " 6   so2_mass_lbs               3586052 non-null  float32            \n",
      " 7   so2_mass_measurement_code  3586052 non-null  string             \n",
      " 8   nox_rate_lbs_mmbtu         3716001 non-null  float32            \n",
      " 9   nox_rate_measurement_code  3716001 non-null  string             \n",
      " 10  nox_mass_lbs               3716549 non-null  float32            \n",
      " 11  nox_mass_measurement_code  3716549 non-null  string             \n",
      " 12  co2_mass_tons              3688397 non-null  float32            \n",
      " 13  co2_mass_measurement_code  3688397 non-null  string             \n",
      " 14  heat_content_mmbtu         8006424 non-null  float32            \n",
      " 15  facility_id                8006424 non-null  Int32              \n",
      " 16  unit_id_epa                8006424 non-null  string             \n",
      " 17  year                       8006424 non-null  Int32              \n",
      " 18  state                      8006424 non-null  category           \n",
      "dtypes: Int32(3), category(1), datetime64[ns, UTC](1), float32(8), string(6)\n",
      "memory usage: 2.9 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.info(show_counts=True, memory_usage=\"deep\"))\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Via Intake Catalog\n",
    "* This takes about 15 seconds, and results in the 3GB dataframe as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.2 s, sys: 2.86 s, total: 19.1 s\n",
      "Wall time: 13.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "os.environ[\"INTAKE_PATH\"] = str(INTAKE_PATH_LOCAL)\n",
    "pudl_cat = open_catalog(pudl_catalog_path)\n",
    "source = pudl_cat.epacems_multi_file(\n",
    "    filters=TEST_FILTERS,\n",
    "    engine=\"pyarrow\",\n",
    ")\n",
    "dd = source.to_dask()\n",
    "df = dd.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8006424 entries, 0 to 3320279\n",
      "Data columns (total 17 columns):\n",
      " #   Column                     Non-Null Count    Dtype              \n",
      "---  ------                     --------------    -----              \n",
      " 0   plant_id_eia               8006424 non-null  Int64              \n",
      " 1   unitid                     8006424 non-null  string             \n",
      " 2   operating_datetime_utc     8006424 non-null  datetime64[ns, UTC]\n",
      " 3   operating_time_hours       8003928 non-null  float32            \n",
      " 4   gross_load_mw              8006424 non-null  float32            \n",
      " 5   steam_load_1000_lbs        33252 non-null    float32            \n",
      " 6   so2_mass_lbs               3586052 non-null  float32            \n",
      " 7   so2_mass_measurement_code  3586052 non-null  string             \n",
      " 8   nox_rate_lbs_mmbtu         3716001 non-null  float32            \n",
      " 9   nox_rate_measurement_code  3716001 non-null  string             \n",
      " 10  nox_mass_lbs               3716549 non-null  float32            \n",
      " 11  nox_mass_measurement_code  3716549 non-null  string             \n",
      " 12  co2_mass_tons              3688397 non-null  float32            \n",
      " 13  co2_mass_measurement_code  3688397 non-null  string             \n",
      " 14  heat_content_mmbtu         8006424 non-null  float32            \n",
      " 15  facility_id                8006424 non-null  Int64              \n",
      " 16  unit_id_epa                8006424 non-null  string             \n",
      "dtypes: Int64(2), datetime64[ns, UTC](1), float32(8), string(6)\n",
      "memory usage: 2.9 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.info(show_counts=True, memory_usage=\"deep\"))\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi File Remote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct access with `read_parquet()`\n",
    "* With the `gs://` URL this takes **1 minute** and downloads minimal data.\n",
    "* With the `https://` URL this results in a 403 Forbidden error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.7 s, sys: 3.92 s, total: 29.7 s\n",
      "Wall time: 59.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_parquet(\n",
    "    remote_multi_file,\n",
    "    engine=\"pyarrow\",\n",
    "    filters=TEST_FILTERS,\n",
    "    use_nullable_dtypes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8006424 entries, 0 to 3320279\n",
      "Data columns (total 17 columns):\n",
      " #   Column                     Non-Null Count    Dtype              \n",
      "---  ------                     --------------    -----              \n",
      " 0   plant_id_eia               8006424 non-null  Int64              \n",
      " 1   unitid                     8006424 non-null  string             \n",
      " 2   operating_datetime_utc     8006424 non-null  datetime64[ns, UTC]\n",
      " 3   operating_time_hours       8003928 non-null  float32            \n",
      " 4   gross_load_mw              8006424 non-null  float32            \n",
      " 5   steam_load_1000_lbs        33252 non-null    float32            \n",
      " 6   so2_mass_lbs               3586052 non-null  float32            \n",
      " 7   so2_mass_measurement_code  3586052 non-null  string             \n",
      " 8   nox_rate_lbs_mmbtu         3716001 non-null  float32            \n",
      " 9   nox_rate_measurement_code  3716001 non-null  string             \n",
      " 10  nox_mass_lbs               3716549 non-null  float32            \n",
      " 11  nox_mass_measurement_code  3716549 non-null  string             \n",
      " 12  co2_mass_tons              3688397 non-null  float32            \n",
      " 13  co2_mass_measurement_code  3688397 non-null  string             \n",
      " 14  heat_content_mmbtu         8006424 non-null  float32            \n",
      " 15  facility_id                8006424 non-null  Int64              \n",
      " 16  unit_id_epa                8006424 non-null  string             \n",
      "dtypes: Int64(2), datetime64[ns, UTC](1), float32(8), string(6)\n",
      "memory usage: 2.9 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.info(show_counts=True, memory_usage=\"deep\"))\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Via Intake Catalog\n",
    "* With the `gs://` URL this takes **3 minutes** and downloads a little bit of data across the whole time.\n",
    "* With the `https://` URL this results in a 403 Forbidden error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 12s, sys: 8.69 s, total: 1min 20s\n",
      "Wall time: 2min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "os.environ[\"INTAKE_PATH\"] = INTAKE_PATH_REMOTE\n",
    "pudl_cat = open_catalog(pudl_catalog_path)\n",
    "source = pudl_cat.epacems_multi_file(\n",
    "    filters=TEST_FILTERS,\n",
    "    engine=\"pyarrow\",\n",
    ")\n",
    "dd = source.to_dask()\n",
    "df = dd.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.info(show_counts=True, memory_usage=\"deep\"))\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "* Unsurprisingly, local access is blazing fast regardless of whether it's a single file or many, and while the Intake catalog access takes around 3x as long, it seems fast enough to be plenty usable.\n",
    "* Remote performance using a single file, the `gs://` protocol, and `read_parquet()` was shockingly fast. It took less than 10x as long as direct local access.\n",
    "* Remote performance over `https://` was painfully slow, to the point of being unusable in all uses of Intake. It also seemed to be transmitting far, far more data than in the `gs://` case.\n",
    "* Basically none of the `https://` cases were usable. The only one that completed took 10 minutes.\n",
    "* The only remote Intake catalog case that worked was the single-file `gs://`, which (as with the local catalogs) took about 3x as long as the `read_parquet()` case.\n",
    "* Over `https://` it seems like we can't use directories or wildcards -- we have to enumerate each filename specifically.\n",
    "* Some of the issues here have to be network speed, but I have 50-100Mbit download speeds, and the amount of data being transmitted varied widely between the different cases.\n",
    "* Still some data type issues happening in all of the Intake cases. Strings get turned into objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
